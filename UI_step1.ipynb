{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use this script**\n",
    "\n",
    "/!\\ To run this script, you need a working version of python with **numpy** and **panda** libraries\n",
    "\n",
    "1. Compile the cell of **import libraries**\n",
    "2. Fill the **user defined** part as follows, and compile it :\n",
    "    - For step1_in and step2_in :\n",
    "        * put the names of the different countries/regions in the list countries\n",
    "        * put the path to corresponding data files (those files must follow the template file -> see...)\n",
    "        * set the number of timeseries which have a WEIGHT defined in N_ts variable\n",
    "        * set the number of TD to compute\n",
    "    - For step2_in only : \n",
    "        * put the path of the output of STEP_1 : TD_of_days.out\n",
    "3. **Compile the functions**\n",
    "4. **Call the functions**\n",
    "    - REM : \n",
    "        * before to call the functions, one must close all the DATA.xls\n",
    "        * before calling the function step2_in, one must run the clustering method of STEP_1\n",
    "        * for clustering method don't forget to update the set DIMENSIONS in the TD_main.mod and to delete the TD_of_days.out file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recipe to add a timeseries :\n",
    "> * add the times series columns into the DATA.xlsx in sheet '1.1 Time Series' (also better to update the sheet 1.2 c_p_t but it will work without it)\n",
    "> * if this timeseries should be used for TD clustering,\n",
    ">   * add a line to the weights in sheet '2.2 User defined' with the exact same name as the column.\n",
    ">   * change N_ts value in this script\n",
    ">   * add the names of the ts and its coresponding parameter either to EUD_params, or RES_params, or RES_mult_params according to which category it corresponds\n",
    "> * Run the script according to the explanations here above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T14:58:47.140346Z",
     "start_time": "2021-11-29T14:58:45.179827Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T14:58:48.359649Z",
     "start_time": "2021-11-29T14:58:47.179921Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T15:12:00.839205Z",
     "start_time": "2021-11-29T15:12:00.823581Z"
    }
   },
   "source": [
    "## Data for reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T15:01:40.843174Z",
     "start_time": "2021-11-29T15:01:40.831916Z"
    }
   },
   "outputs": [],
   "source": [
    "## for step1_in and step2_in ##\n",
    "countries = ['NO', 'CN', 'SO'] # countries list\n",
    "data = ['Data\\\\DATA_NO_3C.xlsx', 'Data\\\\DATA_CN_3C.xlsx', 'Data\\\\DATA_SO_3C.xlsx'] # data path (in same order as data path)\n",
    "N_ts = 7 # number of timeseries with a WEIGHT defined (per country)\n",
    "Nbr_TD = 12 # number of typical day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T15:01:55.994509Z",
     "start_time": "2021-11-29T15:01:55.985370Z"
    }
   },
   "outputs": [],
   "source": [
    "## for step2_in ##\n",
    "step1_out = 'STEP_1_TD_selection\\TD_of_days.out'\n",
    "## name of timeseries in DATA.xlsx and corresponding name in ESTD data file\n",
    "# for EUD timeseries\n",
    "EUD_params = {'Electricity (%_elec)' : 'param electricity_time_series :=', 'Space Heating (%_sh)' : 'param heating_time_series :=', 'Space Cooling (%_sc)':'param cooling_time_series :=', 'Passanger mobility (%_pass)' : 'param mob_pass_time_series :=', 'Freight mobility (%_freight)' : 'param mob_freight_time_series :='}\n",
    "# for resources timeseries that have only 1 tech linked to it\n",
    "RES_params = {'PV' : 'PV', 'Wind_offshore' : 'WIND_OFFSHORE', 'Wind_onshore': 'WIND_ONSHORE'}\n",
    "# for resources timeseries that have several techs linked to it\n",
    "RES_mult_params = {'HYDRO_DAM' : ['HYDRO_DAM'], 'HYDRO_RIVER' : ['HYDRO_RIVER'], 'SOLAR' : ['DHN_SOLAR', 'DEC_SOLAR']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From DATA to STEP_1_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T15:14:27.088146Z",
     "start_time": "2021-11-29T15:14:27.056897Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def step1_in(out_path,countries,data, N_ts=6, Nbr_TD=12):\n",
    "    \"step1_in reads the datas at the path of data and prints the Ndata param in Ndata.tsv and the list of timeseries used in this Ndata with their weights and norm\"\n",
    "    N_c = len(data) #number of countries\n",
    "    \n",
    "    ## READ FIRST COUNTRY ##\n",
    "    #reading the weights\n",
    "    weights = pd.read_excel(data[0],  sheet_name='2.2 User defined',  header=[2], index_col=0, nrows = N_ts, usecols = [0,1] ).rename(columns={'Unnamed: 1':'Weights'})\n",
    "    weights.index.rename('Category', inplace=True)\n",
    "    # from weights, create the names of ts for STEP1 and number of ts\n",
    "    ts_names = list(weights.index)\n",
    "\n",
    "    #reading the timeseries\n",
    "    timeseries =  pd.read_excel(data[0],  sheet_name='1.1 Time Series',  header=[1], index_col=0, nrows = 8760 )\n",
    "    timeseries.drop(labels='period_duration [h]', axis=1, inplace=True)\n",
    "\n",
    "    # adding the country suffix\n",
    "    ts_names_all = [str(x)+'_'+countries[0] for x in ts_names]\n",
    "    weights.index =  [str(line) + '_' + countries[0] for line in weights.index]\n",
    "    timeseries.columns = [str(col) + '_' + countries[0] for col in timeseries.columns]\n",
    "    \n",
    "    ## READING THE OTHER COUNTRIES ##\n",
    "    for i in np.arange(1,N_c):\n",
    "        #reading the weights\n",
    "        weights2 = pd.read_excel(data[i],  sheet_name='2.2 User defined',  header=[2], index_col=0, nrows = N_ts, usecols = [0,1] ).rename(columns={'Unnamed: 1':'Weights'})\n",
    "        weights2.index =  [str(line) + '_' + countries[i] for line in weights2.index]\n",
    "        weights = pd.concat([weights, weights2]) #weights = weights.merge(pd.read_excel(data[i],  sheet_name='2.2 User defined',  header=[4], index_col=0, nrows = N_ts, usecols = [0,1] ).rename(columns={'Unnamed: 1':'Weights_'+countries[i]}), left_index=True, right_index=True)\n",
    "        # adding the names of each country's columns that have a weight\n",
    "        ts_names_all = ts_names_all + [str(x)+'_'+countries[i] for x in ts_names]\n",
    "        #reading the timeseries\n",
    "        ts2 = pd.read_excel(data[i],  sheet_name='1.1 Time Series',  header=[1], index_col=0, nrows = 8760).drop(labels='period_duration [h]', axis=1)\n",
    "        ts2.columns = [str(col) + '_' + countries[i] for col in ts2.columns]\n",
    "        timeseries = timeseries.merge(ts2, left_index=True, right_index=True)\n",
    "    \n",
    "    ## NORMALIZING TIMESERIES ##\n",
    "    #compute norm = sum(ts)\n",
    "    norm = timeseries.sum(axis=0)\n",
    "    norm.index.rename('Category', inplace=True)\n",
    "    norm.name = 'Norm'\n",
    "    # normalise ts to have sum(norm_ts)=1\n",
    "    norm_ts = timeseries/norm\n",
    "    # fill NaN with 0\n",
    "    norm_ts.fillna(0, inplace=True)\n",
    "    \n",
    "    ## WEIGHTING TIMESERIES ##\n",
    "    weights = pd.Series(data=weights['Weights'], index=weights.index)\n",
    "    weights.index.rename('Category', inplace=True)\n",
    "    # select columns of ts that matters for STEP1\n",
    "    weight_ts = norm_ts[ts_names_all]\n",
    "    # multiply each timeserie by its weight\n",
    "    weight_ts = weight_ts*weights\n",
    "    \n",
    "    ## CREATING DAY AND HOUR COLUMNS (for later pivoting) ##\n",
    "    # creating df with 2 columns : day of the year | hour in the day\n",
    "    day_and_hour_array = np.ones((24*365,2))\n",
    "    for i in range(365):\n",
    "        day_and_hour_array[i*24:(i+1)*24,0] = day_and_hour_array[i*24:(i+1)*24,0]*(i+1)\n",
    "        day_and_hour_array[i*24:(i+1)*24,1] = np.arange(1,25,1)\n",
    "    day_and_hour = pd.DataFrame(day_and_hour_array, index = np.arange(1,8761,1), columns=['D_of_H','H_of_D'])\n",
    "    day_and_hour = day_and_hour.astype('int64')\n",
    "    # merge day_and_hour with weight_ts for later pivot\n",
    "    weight_ts = weight_ts.merge(day_and_hour,left_index=True, right_index=True)\n",
    "    \n",
    "    ## CREATING NDATA ##\n",
    "    # pivoting timeseries to get Ndata (but not normalised and weighted)\n",
    "    Ndata = weight_ts.pivot(index='D_of_H', columns='H_of_D', values=ts_names_all)\n",
    "    # renumeroting Ndata columns\n",
    "    Ndata.columns = np.arange(1,24*N_ts*N_c+1)\n",
    "    # adding AMPL syntax for param Ndata\n",
    "    Ndata.rename(columns = {Ndata.shape[1]:str(Ndata.shape[1])+' '+':='}, inplace=True)\n",
    "    \n",
    "    ## PRINTING NDATA AND WEIGHTS AND NORMS ##\n",
    "   \n",
    "    \n",
    "    with open(out_path, mode='w',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        TD_writer.writerow(['# -------------------------------------------------------------------------------------------------------------------------\t'])\n",
    "        TD_writer.writerow(['#\tEnergyScope TD is an open-source energy model suitable for country scale analysis. It is a simplified representation of an urban or national energy system accounting for the energy flows'])\n",
    "        TD_writer.writerow(['#\twithin its boundaries. Based on a hourly resolution, it optimises the design and operation of the energy system while minimizing the cost of the system.'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tCopyright (C) <2018-2019> <Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland and Université catholique de Louvain (UCLouvain), Belgium>'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tLicensed under the Apache License, Version 2.0 (the \"License\");'])\n",
    "        TD_writer.writerow(['#\tyou may not use this file except in compliance with the License.'])\n",
    "        TD_writer.writerow(['#\tYou may obtain a copy of the License at'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\thttp://www.apache.org/licenses/LICENSE-2.0'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tUnless required by applicable law or agreed to in writing, software'])\n",
    "        TD_writer.writerow(['#\tdistributed under the License is distributed on an \"AS IS\" BASIS,'])\n",
    "        TD_writer.writerow(['#\tWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.'])\n",
    "        TD_writer.writerow(['#\tSee the License for the specific language governing permissions and'])\n",
    "        TD_writer.writerow(['#\tlimitations under the License.'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tDescription and complete License: see LICENSE file.'])\n",
    "        TD_writer.writerow(['# -------------------------------------------------------------------------------------------------------------------------\t'])\n",
    "        TD_writer.writerow(['\t'])\n",
    "        TD_writer.writerow(['# SETS depending on TD\t'])\n",
    "        TD_writer.writerow(['# --------------------------\t'])\n",
    "        TD_writer.writerow(['param Nbr_TD :=\t'+str(Nbr_TD)])\n",
    "        TD_writer.writerow([';\t\t'])\n",
    "        TD_writer.writerow(['\t\t'])\n",
    "        \n",
    "    # concatenating and printing weights and norm\n",
    "    weight_norm = pd.concat([weights, norm[ts_names_all]], axis=1)\n",
    "    weight_norm.reset_index(inplace=True)\n",
    "    weight_norm['#'] = '#'\n",
    "    weight_norm = weight_norm[['#', 'Category', 'Weights', 'Norm']]\n",
    "    weight_norm.to_csv(out_path, sep='\\t', header=True, index=False, mode='a')\n",
    "    \n",
    "    with open(out_path, mode='a',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "        TD_writer.writerow([''])\n",
    "    \n",
    "    # printing param Ndata\n",
    "    Ndata.to_csv(out_path, sep='\\t', header=True, index=True, index_label='param Ndata :', mode='a')\n",
    "    \n",
    "    with open(out_path, mode='a',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "        TD_writer.writerow([';'])\n",
    "    \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From STEP_1_out to STEP_2_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T15:00:36.343603Z",
     "start_time": "2021-11-29T15:00:36.257872Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def step2_in(out_path, countries, data, step1_out, EUD_params, RES_params, RES_mult_params, N_ts=6, Nbr_TD=12):\n",
    "    ## READING OUTPUT OF STEP1 ##\n",
    "    TD_of_days = pd.read_csv(step1_out, names=['TD_of_days'])\n",
    "    TD_of_days['day'] = np.arange(1,366,1) # putting the days of the year beside\n",
    "    \n",
    "    ## COMPUTING NUMBER OF DAYS REPRESENTED BY EACH TD ##\n",
    "    sorted_TD = TD_of_days.groupby('TD_of_days').count()\n",
    "    sorted_TD.rename(columns={'day':'#days'}, inplace=True)\n",
    "    sorted_TD.reset_index(inplace=True)\n",
    "    sorted_TD.set_index(np.arange(1,Nbr_TD+1), inplace=True) # adding number of TD as index\n",
    "    \n",
    "    ## BUILDING T_H_TD MATRICE ##\n",
    "    # generate T_H_TD\n",
    "    TD_and_hour_array = np.ones((24*365,2))\n",
    "    for i in range(365):\n",
    "        TD_and_hour_array[i*24:(i+1)*24,0] = np.arange(1,25,1)\n",
    "        TD_and_hour_array[i*24:(i+1)*24,1] = TD_and_hour_array[i*24:(i+1)*24,1]*sorted_TD[sorted_TD['TD_of_days']==TD_of_days.loc[i,'TD_of_days']].index.values\n",
    "    T_H_TD = pd.DataFrame(TD_and_hour_array, index = np.arange(1,8761,1), columns=['H_of_D','TD_of_day'])\n",
    "    T_H_TD = T_H_TD.astype('int64')\n",
    "    # giving the right syntax\n",
    "    T_H_TD.reset_index(inplace=True)\n",
    "    T_H_TD.rename(columns={'index':'H_of_Y'}, inplace=True)\n",
    "    T_H_TD['par_g'] = '('\n",
    "    T_H_TD['par_d'] = ')'\n",
    "    T_H_TD['comma1'] = ','\n",
    "    T_H_TD['comma2'] = ','\n",
    "    # giving the right order to the columns\n",
    "    T_H_TD = T_H_TD[['par_g','H_of_Y','comma1','H_of_D','comma2','TD_of_day','par_d']]\n",
    "    \n",
    "    ## READING THE TIMESERIES IN DATA FILE ##\n",
    "    N_c = len(data) #number of countries\n",
    "    # READ FIRST COUNTRY #\n",
    "    #reading the timeseries\n",
    "    timeseries =  pd.read_excel(data[0],  sheet_name='1.1 Time Series',  header=[1], index_col=0, nrows = 8760 )\n",
    "    timeseries.drop(labels='period_duration [h]', axis=1, inplace=True)\n",
    "    ts_names = list(timeseries.columns) # names of the columns\n",
    "    timeseries.columns = [str(col) + '_' + countries[0] for col in timeseries.columns]\n",
    "\n",
    "    # READING THE OTHER COUNTRIES #\n",
    "    for i in np.arange(1,N_c):\n",
    "        #reading the timeseries\n",
    "        ts2 = pd.read_excel(data[i],  sheet_name='1.1 Time Series',  header=[1], index_col=0, nrows = 8760).drop(labels='period_duration [h]', axis=1)\n",
    "        ts2.columns = [str(col) + '_' + countries[i] for col in ts2.columns]\n",
    "        timeseries = timeseries.merge(ts2, left_index=True, right_index=True)\n",
    "\n",
    "    # COMPUTING THE NORM OVER THE YEAR ##\n",
    "    norm = timeseries.sum(axis=0)\n",
    "    norm.index.rename('Category', inplace=True)\n",
    "    norm.name = 'Norm'\n",
    "    \n",
    "    ## BUILDING TD TIMESERIES ##\n",
    "    # creating df with 2 columns : day of the year | hour in the day\n",
    "    day_and_hour_array = np.ones((24*365,2))\n",
    "    for i in range(365):\n",
    "        day_and_hour_array[i*24:(i+1)*24,0] = day_and_hour_array[i*24:(i+1)*24,0]*(i+1)\n",
    "        day_and_hour_array[i*24:(i+1)*24,1] = np.arange(1,25,1)\n",
    "    day_and_hour = pd.DataFrame(day_and_hour_array, index = np.arange(1,8761,1), columns=['D_of_H','H_of_D'])\n",
    "    day_and_hour = day_and_hour.astype('int64')\n",
    "    timeseries = timeseries.merge(day_and_hour,left_index=True, right_index=True)\n",
    "\n",
    "    #selecting timeseries of TD only\n",
    "    TD_ts = timeseries[timeseries['D_of_H'].isin(sorted_TD['TD_of_days'])]\n",
    "    \n",
    "    ## COMPUTING THE NORM_TD OVER THE YEAR FOR CORRECTION ##\n",
    "    # computing the sum of ts over each TD\n",
    "    agg_TD_ts = TD_ts.groupby('D_of_H').sum()\n",
    "    agg_TD_ts.reset_index(inplace=True)\n",
    "    agg_TD_ts.set_index(np.arange(1,Nbr_TD+1), inplace=True)\n",
    "    agg_TD_ts.drop(columns=['D_of_H','H_of_D'], inplace=True)\n",
    "    # multiplicating each TD by the number of day it represents\n",
    "    for c in agg_TD_ts.columns:\n",
    "        agg_TD_ts[c] = agg_TD_ts[c]*sorted_TD['#days']\n",
    "    # sum of new ts over the whole year\n",
    "    norm_TD = agg_TD_ts.sum()\n",
    "    \n",
    "    ## BUILDING THE DF WITH THE TS OF EACH TD FOR EACH CATEGORY ##\n",
    "    # pivoting TD_ts to obtain a (24,Nbr_TD*Nbr_ts*N_c)\n",
    "    all_TD_ts = TD_ts.pivot(index='H_of_D', columns='D_of_H')\n",
    "    \n",
    "    \n",
    "    ## PRINTING ##\n",
    "    # printing description of file\n",
    "    with open(out_path, mode='w',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        TD_writer.writerow(['# -------------------------------------------------------------------------------------------------------------------------\t'])\n",
    "        TD_writer.writerow(['#\tEnergyScope TD is an open-source energy model suitable for country scale analysis. It is a simplified representation of an urban or national energy system accounting for the energy flows'])\n",
    "        TD_writer.writerow(['#\twithin its boundaries. Based on a hourly resolution, it optimises the design and operation of the energy system while minimizing the cost of the system.'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tCopyright (C) <2018-2019> <Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland and Université catholique de Louvain (UCLouvain), Belgium>'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tLicensed under the Apache License, Version 2.0 (the \"License\");'])\n",
    "        TD_writer.writerow(['#\tyou may not use this file except in compliance with the License.'])\n",
    "        TD_writer.writerow(['#\tYou may obtain a copy of the License at'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\thttp://www.apache.org/licenses/LICENSE-2.0'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tUnless required by applicable law or agreed to in writing, software'])\n",
    "        TD_writer.writerow(['#\tdistributed under the License is distributed on an \"AS IS\" BASIS,'])\n",
    "        TD_writer.writerow(['#\tWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.'])\n",
    "        TD_writer.writerow(['#\tSee the License for the specific language governing permissions and'])\n",
    "        TD_writer.writerow(['#\tlimitations under the License.'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tDescription and complete License: see LICENSE file.'])\n",
    "        TD_writer.writerow(['# -------------------------------------------------------------------------------------------------------------------------\t'])\n",
    "        TD_writer.writerow(['\t'])\n",
    "        TD_writer.writerow(['# SETS depending on TD\t'])\n",
    "        TD_writer.writerow(['# --------------------------\t'])\n",
    "        TD_writer.writerow(['param peak_sh_factor\t:=\t'])\n",
    "        peak_sh_factor = 1\n",
    "        for c in countries:\n",
    "            max_sh_TD = TD_ts.loc[:,'Space Heating (%_sh)_'+c].max()\n",
    "            max_sh_all = timeseries.loc[:,'Space Heating (%_sh)_'+c].max()\n",
    "            peak_sh_factor = max_sh_all/max_sh_TD\n",
    "            TD_writer.writerow([c + ' ' + str(peak_sh_factor)])\n",
    "        TD_writer.writerow([';\t\t'])\n",
    "        TD_writer.writerow(['\t\t'])\n",
    "        TD_writer.writerow(['#SETS [Figure 3]\t\t'])\n",
    "        TD_writer.writerow(['set T_H_TD := \t\t'])\n",
    "    \n",
    "    # printing T_H_TD param\n",
    "    T_H_TD.to_csv(out_path,sep='\\t', header=False, index=False, mode='a', quoting=csv.QUOTE_NONE)\n",
    "    \n",
    "    # printing interlude\n",
    "    with open(out_path, mode='a',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        TD_writer.writerow([';'])\n",
    "        TD_writer.writerow([''])\n",
    "        TD_writer.writerow(['# -----------------------------'])\n",
    "        TD_writer.writerow(['# PARAMETERS DEPENDING ON NUMBER OF TYPICAL DAYS : '])\n",
    "        TD_writer.writerow(['# -----------------------------'])\n",
    "        TD_writer.writerow([''])\n",
    "        \n",
    "    #if only 1 country\n",
    "    if N_c==1:\n",
    "        # printing EUD timeseries param\n",
    "        for l in EUD_params.keys():\n",
    "            with open(out_path, mode='a',newline='') as TD_file:\n",
    "                TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "                TD_writer.writerow([EUD_params[l][0:-1]])    \n",
    "            for c in countries:\n",
    "                name = l+'_'+c\n",
    "                ts = all_TD_ts[name]\n",
    "                ts.columns = np.arange(1,Nbr_TD+1) \n",
    "                ts = ts*norm[name]/norm_TD[name]\n",
    "                ts.fillna(0, inplace=True)\n",
    "                ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='', mode='a', quoting=csv.QUOTE_NONE)\n",
    "            with open(out_path, mode='a',newline='') as TD_file:\n",
    "                TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "                TD_writer.writerow(';')\n",
    "                TD_writer.writerow([''])\n",
    "\n",
    "        # printing c_p_t param #\n",
    "        with open(out_path, mode='a',newline='') as TD_file:\n",
    "            TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "            TD_writer.writerow(['param c_p_t:='])    \n",
    "        # printing c_p_t part where 1 ts => 1 tech\n",
    "        for l in RES_params.keys():\n",
    "            for c in countries:\n",
    "                name = RES_params[l]+'_'+c\n",
    "                ts = all_TD_ts[name]\n",
    "                ts.columns = np.arange(1,Nbr_TD+1) \n",
    "                ts = ts*norm[name]/norm_TD[name]\n",
    "                ts.fillna(0, inplace=True)\n",
    "                ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + RES_params[l] + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "        # printing c_p_t part where 1 ts => more then 1 tech        \n",
    "        for l in RES_mult_params.keys():\n",
    "            for j in RES_mult_params[l]:\n",
    "                for c in countries:\n",
    "                    name = l+'_'+c\n",
    "                    ts = all_TD_ts[name]\n",
    "                    ts.columns = np.arange(1,Nbr_TD+1) \n",
    "                    ts = ts*norm[name]/norm_TD[name]\n",
    "                    ts.fillna(0, inplace=True)\n",
    "                    ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                    ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + j + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        with open(out_path, mode='a',newline='') as TD_file:\n",
    "            TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "            TD_writer.writerow([';'])\n",
    "    else:\n",
    "        # printing EUD timeseries param\n",
    "        for l in EUD_params.keys():\n",
    "            with open(out_path, mode='a',newline='') as TD_file:\n",
    "                TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "                TD_writer.writerow([EUD_params[l]])    \n",
    "            for c in countries:\n",
    "                name = l+'_'+c\n",
    "                ts = all_TD_ts[name]\n",
    "                ts.columns = np.arange(1,Nbr_TD+1) \n",
    "                ts = ts*norm[name]/norm_TD[name]\n",
    "                ts.fillna(0, inplace=True)\n",
    "                ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + c + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "            with open(out_path, mode='a',newline='') as TD_file:\n",
    "                TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "                TD_writer.writerow(';')\n",
    "                TD_writer.writerow([''])\n",
    "\n",
    "        # printing c_p_t param #\n",
    "        with open(out_path, mode='a',newline='') as TD_file:\n",
    "            TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "            TD_writer.writerow(['param c_p_t:='])    \n",
    "        # printing c_p_t part where 1 ts => 1 tech\n",
    "        for l in RES_params.values():\n",
    "            for c in countries:\n",
    "                name = l+'_'+c\n",
    "                ts = all_TD_ts[name]\n",
    "                ts.columns = np.arange(1,Nbr_TD+1) \n",
    "                ts = ts*norm[name]/norm_TD[name]\n",
    "                ts.fillna(0, inplace=True)\n",
    "                ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + l + '\",\"' + c + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "        # printing c_p_t part where 1 ts => more then 1 tech        \n",
    "        for l in RES_mult_params.keys():\n",
    "            for j in RES_mult_params[l]:\n",
    "                for c in countries:\n",
    "                    name = l+'_'+c\n",
    "                    ts = all_TD_ts[name]\n",
    "                    ts.columns = np.arange(1,Nbr_TD+1) \n",
    "                    ts = ts*norm[name]/norm_TD[name]\n",
    "                    ts.fillna(0, inplace=True)\n",
    "                    ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                    ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + j + '\",\"' + c + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        with open(out_path, mode='a',newline='') as TD_file:\n",
    "            TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "            TD_writer.writerow([';'])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Calling the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From DATA to STEP_1_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T15:00:48.810544Z",
     "start_time": "2021-11-29T15:00:48.798289Z"
    }
   },
   "outputs": [],
   "source": [
    "out_path = 'STEP_1_TD_selection\\data.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T15:14:35.533625Z",
     "start_time": "2021-11-29T15:14:30.384831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5.133182764053345 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "step1_in(out_path,countries, data, N_ts, Nbr_TD)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the TD and the number of days they represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T15:17:04.403630Z",
     "start_time": "2021-11-29T15:17:04.366104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TD_of_days</th>\n",
       "      <th>#days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>185</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>199</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>233</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>238</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>268</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>313</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>341</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TD_of_days  #days\n",
       "1            2     36\n",
       "2           55     34\n",
       "3           71     22\n",
       "4          110     21\n",
       "5          185     21\n",
       "6          199     19\n",
       "7          233     41\n",
       "8          238     32\n",
       "9          248      1\n",
       "10         268     63\n",
       "11         313     35\n",
       "12         341     40"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## READING OUTPUT OF STEP1 ##\n",
    "TD_of_days = pd.read_csv(step1_out, names=['TD_of_days'])\n",
    "TD_of_days['day'] = np.arange(1,366,1) # putting the days of the year beside\n",
    "\n",
    "## COMPUTING NUMBER OF DAYS REPRESENTED BY EACH TD ##\n",
    "sorted_TD = TD_of_days.groupby('TD_of_days').count()\n",
    "sorted_TD.rename(columns={'day':'#days'}, inplace=True)\n",
    "sorted_TD.reset_index(inplace=True)\n",
    "sorted_TD.set_index(np.arange(1,Nbr_TD+1), inplace=True) # adding number of TD as index\n",
    "sorted_TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From STEP_1_OUT to STEP_2_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T15:17:31.968616Z",
     "start_time": "2021-11-29T15:17:31.953032Z"
    }
   },
   "outputs": [],
   "source": [
    "## PRINTING 'ES_TD_'+str(Nbr_TD)+'TD.dat' ##\n",
    "out_path = 'ESTD_'+str(Nbr_TD)+'TD_test.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T15:17:39.387769Z",
     "start_time": "2021-11-29T15:17:33.844366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5.527782678604126 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "step2_in(out_path, countries, data, step1_out, EUD_params, RES_params, RES_mult_params, N_ts, Nbr_TD)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## READING OUTPUT OF STEP1 ##\n",
    "TD_of_days = pd.read_csv(step1_out, names=['TD_of_days'])\n",
    "TD_of_days['day'] = np.arange(1,366,1) # putting the days of the year beside\n",
    "\n",
    "## COMPUTING NUMBER OF DAYS REPRESENTED BY EACH TD ##\n",
    "sorted_TD = TD_of_days.groupby('TD_of_days').count()\n",
    "sorted_TD.rename(columns={'day':'#days'}, inplace=True)\n",
    "sorted_TD.reset_index(inplace=True)\n",
    "sorted_TD.set_index(np.arange(1,Nbr_TD+1), inplace=True) # adding number of TD as index\n",
    "\n",
    "## BUILDING T_H_TD MATRICE ##\n",
    "# generate T_H_TD\n",
    "TD_and_hour_array = np.ones((24*365,2))\n",
    "for i in range(365):\n",
    "    TD_and_hour_array[i*24:(i+1)*24,0] = np.arange(1,25,1)\n",
    "    TD_and_hour_array[i*24:(i+1)*24,1] = TD_and_hour_array[i*24:(i+1)*24,1]*sorted_TD[sorted_TD['TD_of_days']==TD_of_days.loc[i,'TD_of_days']].index.values\n",
    "T_H_TD = pd.DataFrame(TD_and_hour_array, index = np.arange(1,8761,1), columns=['H_of_D','TD_of_day'])\n",
    "T_H_TD = T_H_TD.astype('int64')\n",
    "# giving the right syntax\n",
    "T_H_TD.reset_index(inplace=True)\n",
    "T_H_TD.rename(columns={'index':'H_of_Y'}, inplace=True)\n",
    "T_H_TD['par_g'] = '('\n",
    "T_H_TD['par_d'] = ')'\n",
    "T_H_TD['comma1'] = ','\n",
    "T_H_TD['comma2'] = ','\n",
    "# giving the right order to the columns\n",
    "T_H_TD = T_H_TD[['par_g','H_of_Y','comma1','H_of_D','comma2','TD_of_day','par_d']]\n",
    "\n",
    "## READING THE TIMESERIES IN DATA FILE ##\n",
    "N_c = len(data) #number of countries\n",
    "# READ FIRST COUNTRY #\n",
    "#reading the timeseries\n",
    "timeseries =  pd.read_excel(data[0],  sheet_name='1.1 Time Series',  header=[1], index_col=0, nrows = 8760 )\n",
    "timeseries.drop(labels='period_duration [h]', axis=1, inplace=True)\n",
    "ts_names = list(timeseries.columns) # names of the columns\n",
    "timeseries.columns = [str(col) + '_' + countries[0] for col in timeseries.columns]\n",
    "\n",
    "# READING THE OTHER COUNTRIES #\n",
    "for i in np.arange(1,N_c):\n",
    "    #reading the timeseries\n",
    "    ts2 = pd.read_excel(data[i],  sheet_name='1.1 Time Series',  header=[1], index_col=0, nrows = 8760).drop(labels='period_duration [h]', axis=1)\n",
    "    ts2.columns = [str(col) + '_' + countries[i] for col in ts2.columns]\n",
    "    timeseries = timeseries.merge(ts2, left_index=True, right_index=True)\n",
    "\n",
    "# COMPUTING THE NORM OVER THE YEAR ##\n",
    "norm = timeseries.sum(axis=0)\n",
    "norm.index.rename('Category', inplace=True)\n",
    "norm.name = 'Norm'\n",
    "\n",
    "## BUILDING TD TIMESERIES ##\n",
    "# creating df with 2 columns : day of the year | hour in the day\n",
    "day_and_hour_array = np.ones((24*365,2))\n",
    "for i in range(365):\n",
    "    day_and_hour_array[i*24:(i+1)*24,0] = day_and_hour_array[i*24:(i+1)*24,0]*(i+1)\n",
    "    day_and_hour_array[i*24:(i+1)*24,1] = np.arange(1,25,1)\n",
    "day_and_hour = pd.DataFrame(day_and_hour_array, index = np.arange(1,8761,1), columns=['D_of_H','H_of_D'])\n",
    "day_and_hour = day_and_hour.astype('int64')\n",
    "timeseries = timeseries.merge(day_and_hour,left_index=True, right_index=True)\n",
    "\n",
    "#selecting timeseries of TD only\n",
    "TD_ts = timeseries[timeseries['D_of_H'].isin(sorted_TD['TD_of_days'])]\n",
    "\n",
    "## COMPUTING THE NORM_TD OVER THE YEAR FOR CORRECTION ##\n",
    "# computing the sum of ts over each TD\n",
    "agg_TD_ts = TD_ts.groupby('D_of_H').sum()\n",
    "agg_TD_ts.reset_index(inplace=True)\n",
    "agg_TD_ts.set_index(np.arange(1,Nbr_TD+1), inplace=True)\n",
    "agg_TD_ts.drop(columns=['D_of_H','H_of_D'], inplace=True)\n",
    "# multiplicating each TD by the number of day it represents\n",
    "for c in agg_TD_ts.columns:\n",
    "    agg_TD_ts[c] = agg_TD_ts[c]*sorted_TD['#days']\n",
    "# sum of new ts over the whole year\n",
    "norm_TD = agg_TD_ts.sum()\n",
    "\n",
    "## BUILDING THE DF WITH THE TS OF EACH TD FOR EACH CATEGORY ##\n",
    "# pivoting TD_ts to obtain a (24,Nbr_TD*Nbr_ts*N_c)\n",
    "all_TD_ts = TD_ts.pivot(index='H_of_D', columns='D_of_H')\n",
    "\n",
    "\n",
    "## PRINTING ##\n",
    "# printing description of file\n",
    "with open(out_path, mode='w',newline='') as TD_file:\n",
    "    TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    TD_writer.writerow(['# -------------------------------------------------------------------------------------------------------------------------\t'])\n",
    "    TD_writer.writerow(['#\tEnergyScope TD is an open-source energy model suitable for country scale analysis. It is a simplified representation of an urban or national energy system accounting for the energy flows'])\n",
    "    TD_writer.writerow(['#\twithin its boundaries. Based on a hourly resolution, it optimises the design and operation of the energy system while minimizing the cost of the system.'])\n",
    "    TD_writer.writerow(['#\t'])\n",
    "    TD_writer.writerow(['#\tCopyright (C) <2018-2019> <Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland and Université catholique de Louvain (UCLouvain), Belgium>'])\n",
    "    TD_writer.writerow(['#\t'])\n",
    "    TD_writer.writerow(['#\t'])\n",
    "    TD_writer.writerow(['#\tLicensed under the Apache License, Version 2.0 (the \"License\");'])\n",
    "    TD_writer.writerow(['#\tyou may not use this file except in compliance with the License.'])\n",
    "    TD_writer.writerow(['#\tYou may obtain a copy of the License at'])\n",
    "    TD_writer.writerow(['#\t'])\n",
    "    TD_writer.writerow(['#\thttp://www.apache.org/licenses/LICENSE-2.0'])\n",
    "    TD_writer.writerow(['#\t'])\n",
    "    TD_writer.writerow(['#\tUnless required by applicable law or agreed to in writing, software'])\n",
    "    TD_writer.writerow(['#\tdistributed under the License is distributed on an \"AS IS\" BASIS,'])\n",
    "    TD_writer.writerow(['#\tWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.'])\n",
    "    TD_writer.writerow(['#\tSee the License for the specific language governing permissions and'])\n",
    "    TD_writer.writerow(['#\tlimitations under the License.'])\n",
    "    TD_writer.writerow(['#\t'])\n",
    "    TD_writer.writerow(['#\tDescription and complete License: see LICENSE file.'])\n",
    "    TD_writer.writerow(['# -------------------------------------------------------------------------------------------------------------------------\t'])\n",
    "    TD_writer.writerow(['\t'])\n",
    "    TD_writer.writerow(['# SETS depending on TD\t'])\n",
    "    TD_writer.writerow(['# --------------------------\t'])\n",
    "    TD_writer.writerow(['param peak_sh_factor\t:=\t'])\n",
    "    peak_sh_factor = 1\n",
    "    for c in countries:\n",
    "        max_sh_TD = TD_ts.loc[:,'Space Heating (%_sh)_'+c].max()\n",
    "        max_sh_all = timeseries.loc[:,'Space Heating (%_sh)_'+c].max()\n",
    "        peak_sh_factor = max_sh_all/max_sh_TD\n",
    "        TD_writer.writerow([c + ' ' + str(peak_sh_factor)])\n",
    "    TD_writer.writerow([';\t\t'])\n",
    "    TD_writer.writerow(['\t\t'])\n",
    "    TD_writer.writerow(['#SETS [Figure 3]\t\t'])\n",
    "    TD_writer.writerow(['set T_H_TD := \t\t'])\n",
    "\n",
    "# printing T_H_TD param\n",
    "T_H_TD.to_csv(out_path,sep='\\t', header=False, index=False, mode='a', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "# printing interlude\n",
    "with open(out_path, mode='a',newline='') as TD_file:\n",
    "    TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    TD_writer.writerow([';'])\n",
    "    TD_writer.writerow([''])\n",
    "    TD_writer.writerow(['# -----------------------------'])\n",
    "    TD_writer.writerow(['# PARAMETERS DEPENDING ON NUMBER OF TYPICAL DAYS : '])\n",
    "    TD_writer.writerow(['# -----------------------------'])\n",
    "    TD_writer.writerow([''])\n",
    "\n",
    "#if only 1 country\n",
    "if N_c==1:\n",
    "    # printing EUD timeseries param\n",
    "    for l in EUD_params.keys():\n",
    "        with open(out_path, mode='a',newline='') as TD_file:\n",
    "            TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "            TD_writer.writerow([EUD_params[l][0:-1]])    \n",
    "        for c in countries:\n",
    "            name = l+'_'+c\n",
    "            ts = all_TD_ts[name]\n",
    "            ts.columns = np.arange(1,Nbr_TD+1) \n",
    "            ts = ts*norm[name]/norm_TD[name]\n",
    "            ts.fillna(0, inplace=True)\n",
    "            ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "            ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='', mode='a', quoting=csv.QUOTE_NONE)\n",
    "        with open(out_path, mode='a',newline='') as TD_file:\n",
    "            TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "            TD_writer.writerow(';')\n",
    "            TD_writer.writerow([''])\n",
    "\n",
    "    # printing c_p_t param #\n",
    "    with open(out_path, mode='a',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "        TD_writer.writerow(['param c_p_t:='])    \n",
    "    # printing c_p_t part where 1 ts => 1 tech\n",
    "    for l in RES_params.keys():\n",
    "        for c in countries:\n",
    "            name = RES_params[l]+'_'+c\n",
    "            ts = all_TD_ts[name]\n",
    "            ts.columns = np.arange(1,Nbr_TD+1) \n",
    "            ts = ts*norm[name]/norm_TD[name]\n",
    "            ts.fillna(0, inplace=True)\n",
    "            ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "            ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + RES_params[l] + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "    # printing c_p_t part where 1 ts => more then 1 tech        \n",
    "    for l in RES_mult_params.keys():\n",
    "        for j in RES_mult_params[l]:\n",
    "            for c in countries:\n",
    "                name = l+'_'+c\n",
    "                ts = all_TD_ts[name]\n",
    "                ts.columns = np.arange(1,Nbr_TD+1) \n",
    "                ts = ts*norm[name]/norm_TD[name]\n",
    "                ts.fillna(0, inplace=True)\n",
    "                ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + j + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    with open(out_path, mode='a',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "        TD_writer.writerow([';'])\n",
    "# if more than 1 country\n",
    "else:\n",
    "    # printing EUD timeseries param\n",
    "    for l in EUD_params.keys():\n",
    "        with open(out_path, mode='a',newline='') as TD_file:\n",
    "            TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "            TD_writer.writerow([EUD_params[l]])    \n",
    "        for c in countries:\n",
    "            name = l+'_'+c\n",
    "            ts = all_TD_ts[name]\n",
    "            ts.columns = np.arange(1,Nbr_TD+1) \n",
    "            ts = ts*norm[name]/norm_TD[name]\n",
    "            ts.fillna(0, inplace=True)\n",
    "            ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "            ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + c + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "        with open(out_path, mode='a',newline='') as TD_file:\n",
    "            TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "            TD_writer.writerow(';')\n",
    "            TD_writer.writerow([''])\n",
    "\n",
    "    # printing c_p_t param #\n",
    "    with open(out_path, mode='a',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "        TD_writer.writerow(['param c_p_t:='])    \n",
    "    # printing c_p_t part where 1 ts => 1 tech\n",
    "    for l in RES_params.values():\n",
    "        for c in countries:\n",
    "            name = l+'_'+c\n",
    "            ts = all_TD_ts[name]\n",
    "            ts.columns = np.arange(1,Nbr_TD+1) \n",
    "            ts = ts*norm[name]/norm_TD[name]\n",
    "            ts.fillna(0, inplace=True)\n",
    "            ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "            ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + l + '\",\"' + c + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "    # printing c_p_t part where 1 ts => more then 1 tech        \n",
    "    for l in RES_mult_params.keys():\n",
    "        for j in RES_mult_params[l]:\n",
    "            for c in countries:\n",
    "                name = l+'_'+c\n",
    "                ts = all_TD_ts[name]\n",
    "                ts.columns = np.arange(1,Nbr_TD+1) \n",
    "                ts = ts*norm[name]/norm_TD[name]\n",
    "                ts.fillna(0, inplace=True)\n",
    "                ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + j + '\",\"' + c + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    with open(out_path, mode='a',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "        TD_writer.writerow([';'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Electricity (%_elec)_NO               1.000000\n",
       "Space Heating (%_sh)_NO               1.000000\n",
       "Space Cooling (%_sc)_NO               1.000000\n",
       "Passanger mobility (%_pass)_NO        1.000000\n",
       "Freight mobility (%_freight)_NO       1.000000\n",
       "PV_NO                              1445.456000\n",
       "WIND_ONSHORE_NO                    1753.528736\n",
       "WIND_OFFSHORE_NO                      0.876000\n",
       "HYDRO_DAM_NO                       1050.078031\n",
       "HYDRO_RIVER_NO                     3848.955954\n",
       "SOLAR_NO                            756.046087\n",
       "Electricity (%_elec)_CN               1.000000\n",
       "Space Heating (%_sh)_CN               1.000000\n",
       "Space Cooling (%_sc)_CN               1.000000\n",
       "Passanger mobility (%_pass)_CN        1.000000\n",
       "Freight mobility (%_freight)_CN       1.000000\n",
       "PV_CN                              1489.484477\n",
       "WIND_ONSHORE_CN                    2510.598800\n",
       "WIND_OFFSHORE_CN                      0.876000\n",
       "HYDRO_DAM_CN                       1050.078031\n",
       "HYDRO_RIVER_CN                     3848.955954\n",
       "SOLAR_CN                            756.046087\n",
       "Electricity (%_elec)_SO               1.000000\n",
       "Space Heating (%_sh)_SO               1.000000\n",
       "Space Cooling (%_sc)_SO               1.000000\n",
       "Passanger mobility (%_pass)_SO        1.000000\n",
       "Freight mobility (%_freight)_SO       1.000000\n",
       "PV_SO                              1577.041602\n",
       "WIND_ONSHORE_SO                    2628.435049\n",
       "WIND_OFFSHORE_SO                      0.876000\n",
       "HYDRO_DAM_SO                       1050.078031\n",
       "HYDRO_RIVER_SO                     3848.955954\n",
       "SOLAR_SO                            756.046087\n",
       "Name: Norm, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Electricity (%_elec)_NO               0.974373\n",
       "Space Heating (%_sh)_NO               0.948072\n",
       "Space Cooling (%_sc)_NO               0.981310\n",
       "Passanger mobility (%_pass)_NO        1.000000\n",
       "Freight mobility (%_freight)_NO       1.000000\n",
       "PV_NO                              1505.685728\n",
       "WIND_ONSHORE_NO                    1213.552697\n",
       "WIND_OFFSHORE_NO                      0.876000\n",
       "HYDRO_DAM_NO                       1043.062113\n",
       "HYDRO_RIVER_NO                     3406.030653\n",
       "SOLAR_NO                            449.902692\n",
       "Electricity (%_elec)_CN               1.013294\n",
       "Space Heating (%_sh)_CN               0.918729\n",
       "Space Cooling (%_sc)_CN               0.944248\n",
       "Passanger mobility (%_pass)_CN        1.000000\n",
       "Freight mobility (%_freight)_CN       1.000000\n",
       "PV_CN                              1487.410622\n",
       "WIND_ONSHORE_CN                    1965.773381\n",
       "WIND_OFFSHORE_CN                      0.876000\n",
       "HYDRO_DAM_CN                       1043.062113\n",
       "HYDRO_RIVER_CN                     3406.030653\n",
       "SOLAR_CN                            449.902692\n",
       "Electricity (%_elec)_SO               1.026891\n",
       "Space Heating (%_sh)_SO               0.986180\n",
       "Space Cooling (%_sc)_SO               1.118864\n",
       "Passanger mobility (%_pass)_SO        1.000000\n",
       "Freight mobility (%_freight)_SO       1.000000\n",
       "PV_SO                              1555.155204\n",
       "WIND_ONSHORE_SO                    2144.945846\n",
       "WIND_OFFSHORE_SO                      0.876000\n",
       "HYDRO_DAM_SO                       1043.062113\n",
       "HYDRO_RIVER_SO                     3406.030653\n",
       "SOLAR_SO                            449.902692\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_g</th>\n",
       "      <th>H_of_Y</th>\n",
       "      <th>comma1</th>\n",
       "      <th>H_of_D</th>\n",
       "      <th>comma2</th>\n",
       "      <th>TD_of_day</th>\n",
       "      <th>par_d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(</td>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(</td>\n",
       "      <td>3</td>\n",
       "      <td>,</td>\n",
       "      <td>3</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(</td>\n",
       "      <td>4</td>\n",
       "      <td>,</td>\n",
       "      <td>4</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(</td>\n",
       "      <td>5</td>\n",
       "      <td>,</td>\n",
       "      <td>5</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>(</td>\n",
       "      <td>8756</td>\n",
       "      <td>,</td>\n",
       "      <td>20</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>(</td>\n",
       "      <td>8757</td>\n",
       "      <td>,</td>\n",
       "      <td>21</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>(</td>\n",
       "      <td>8758</td>\n",
       "      <td>,</td>\n",
       "      <td>22</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>(</td>\n",
       "      <td>8759</td>\n",
       "      <td>,</td>\n",
       "      <td>23</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>(</td>\n",
       "      <td>8760</td>\n",
       "      <td>,</td>\n",
       "      <td>24</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     par_g  H_of_Y comma1  H_of_D comma2  TD_of_day par_d\n",
       "0        (       1      ,       1      ,          1     )\n",
       "1        (       2      ,       2      ,          1     )\n",
       "2        (       3      ,       3      ,          1     )\n",
       "3        (       4      ,       4      ,          1     )\n",
       "4        (       5      ,       5      ,          1     )\n",
       "...    ...     ...    ...     ...    ...        ...   ...\n",
       "8755     (    8756      ,      20      ,          1     )\n",
       "8756     (    8757      ,      21      ,          1     )\n",
       "8757     (    8758      ,      22      ,          1     )\n",
       "8758     (    8759      ,      23      ,          1     )\n",
       "8759     (    8760      ,      24      ,          1     )\n",
       "\n",
       "[8760 rows x 7 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_H_TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_H_TD_Borasio = pd.read_csv('T_H_TD_Borasio.txt', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in T_H_TD_Borasio.index:\n",
    "    T_H_TD_Borasio.loc[t,'SOLAR'] =  TD_solar_NO_Borasio.loc[T_H_TD_Borasio.loc[t,'H']-1,T_H_TD_Borasio.loc[t,'TD']-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1edbd405cd0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(np.arange(0,8760,1),solar_NO.loc[:,'SOLAR'].sort_values(ascending=False), color='b')\n",
    "ax.plot(np.arange(0,8760,1),T_H_TD_Borasio.loc[:,'SOLAR'].sort_values(ascending=False), color='r')\n",
    "ax.plot(np.arange(0,8760,1),timeseries.loc[:,'SOLAR_NO'].sort_values(ascending=False), color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0.000000\n",
       "SOLAR    756.046087\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solar_NO.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Electricity (%_elec)_NO</th>\n",
       "      <th>Space Heating (%_sh)_NO</th>\n",
       "      <th>Space Cooling (%_sc)_NO</th>\n",
       "      <th>Passanger mobility (%_pass)_NO</th>\n",
       "      <th>Freight mobility (%_freight)_NO</th>\n",
       "      <th>PV_NO</th>\n",
       "      <th>WIND_ONSHORE_NO</th>\n",
       "      <th>WIND_OFFSHORE_NO</th>\n",
       "      <th>HYDRO_DAM_NO</th>\n",
       "      <th>HYDRO_RIVER_NO</th>\n",
       "      <th>...</th>\n",
       "      <th>Passanger mobility (%_pass)_SO</th>\n",
       "      <th>Freight mobility (%_freight)_SO</th>\n",
       "      <th>PV_SO</th>\n",
       "      <th>WIND_ONSHORE_SO</th>\n",
       "      <th>WIND_OFFSHORE_SO</th>\n",
       "      <th>HYDRO_DAM_SO</th>\n",
       "      <th>HYDRO_RIVER_SO</th>\n",
       "      <th>SOLAR_SO</th>\n",
       "      <th>D_of_H</th>\n",
       "      <th>H_of_D</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{PERIODS}</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.362776</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.219106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.731412</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.219106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.395756</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.196958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.769478</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.196958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.395756</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.008159</td>\n",
       "      <td>0.183511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.827791</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.008159</td>\n",
       "      <td>0.183511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.428736</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.007950</td>\n",
       "      <td>0.175827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834400</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.007950</td>\n",
       "      <td>0.175827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.461715</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.173228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.853630</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.173228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032980</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.315900</td>\n",
       "      <td>0.329281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041520</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.315900</td>\n",
       "      <td>0.329281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032980</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.280545</td>\n",
       "      <td>0.282499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046676</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.280545</td>\n",
       "      <td>0.282499</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065959</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.154498</td>\n",
       "      <td>0.234813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042982</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.154498</td>\n",
       "      <td>0.234813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065959</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.048222</td>\n",
       "      <td>0.180121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037774</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.048222</td>\n",
       "      <td>0.180121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8760</th>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065959</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.023222</td>\n",
       "      <td>0.155827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037774</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.023222</td>\n",
       "      <td>0.155827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Electricity (%_elec)_NO  Space Heating (%_sh)_NO  \\\n",
       "{PERIODS}                                                     \n",
       "1                         0.000072                 0.000430   \n",
       "2                         0.000068                 0.000417   \n",
       "3                         0.000064                 0.000428   \n",
       "4                         0.000061                 0.000432   \n",
       "5                         0.000059                 0.000430   \n",
       "...                            ...                      ...   \n",
       "8756                      0.000106                 0.000285   \n",
       "8757                      0.000103                 0.000299   \n",
       "8758                      0.000093                 0.000308   \n",
       "8759                      0.000085                 0.000317   \n",
       "8760                      0.000078                 0.000328   \n",
       "\n",
       "           Space Cooling (%_sc)_NO  Passanger mobility (%_pass)_NO  \\\n",
       "{PERIODS}                                                            \n",
       "1                              0.0                        0.000006   \n",
       "2                              0.0                        0.000002   \n",
       "3                              0.0                        0.000001   \n",
       "4                              0.0                        0.000014   \n",
       "5                              0.0                        0.000038   \n",
       "...                            ...                             ...   \n",
       "8756                           0.0                        0.000088   \n",
       "8757                           0.0                        0.000059   \n",
       "8758                           0.0                        0.000038   \n",
       "8759                           0.0                        0.000023   \n",
       "8760                           0.0                        0.000009   \n",
       "\n",
       "           Freight mobility (%_freight)_NO     PV_NO  WIND_ONSHORE_NO  \\\n",
       "{PERIODS}                                                               \n",
       "1                                 0.000114  0.000188         0.362776   \n",
       "2                                 0.000114  0.000188         0.395756   \n",
       "3                                 0.000114  0.000188         0.395756   \n",
       "4                                 0.000114  0.000188         0.428736   \n",
       "5                                 0.000114  0.000188         0.461715   \n",
       "...                                    ...       ...              ...   \n",
       "8756                              0.000114  0.000000         0.032980   \n",
       "8757                              0.000114  0.000000         0.032980   \n",
       "8758                              0.000114  0.000000         0.065959   \n",
       "8759                              0.000114  0.000000         0.065959   \n",
       "8760                              0.000114  0.000000         0.065959   \n",
       "\n",
       "           WIND_OFFSHORE_NO  HYDRO_DAM_NO  HYDRO_RIVER_NO  ...  \\\n",
       "{PERIODS}                                                  ...   \n",
       "1                    0.0001      0.033787        0.219106  ...   \n",
       "2                    0.0001      0.010146        0.196958  ...   \n",
       "3                    0.0001      0.008159        0.183511  ...   \n",
       "4                    0.0001      0.007950        0.175827  ...   \n",
       "5                    0.0001      0.008054        0.173228  ...   \n",
       "...                     ...           ...             ...  ...   \n",
       "8756                 0.0001      0.315900        0.329281  ...   \n",
       "8757                 0.0001      0.280545        0.282499  ...   \n",
       "8758                 0.0001      0.154498        0.234813  ...   \n",
       "8759                 0.0001      0.048222        0.180121  ...   \n",
       "8760                 0.0001      0.023222        0.155827  ...   \n",
       "\n",
       "           Passanger mobility (%_pass)_SO  Freight mobility (%_freight)_SO  \\\n",
       "{PERIODS}                                                                    \n",
       "1                                0.000006                         0.000114   \n",
       "2                                0.000002                         0.000114   \n",
       "3                                0.000001                         0.000114   \n",
       "4                                0.000014                         0.000114   \n",
       "5                                0.000038                         0.000114   \n",
       "...                                   ...                              ...   \n",
       "8756                             0.000088                         0.000114   \n",
       "8757                             0.000059                         0.000114   \n",
       "8758                             0.000038                         0.000114   \n",
       "8759                             0.000023                         0.000114   \n",
       "8760                             0.000009                         0.000114   \n",
       "\n",
       "           PV_SO  WIND_ONSHORE_SO  WIND_OFFSHORE_SO  HYDRO_DAM_SO  \\\n",
       "{PERIODS}                                                           \n",
       "1            0.0         0.731412            0.0001      0.033787   \n",
       "2            0.0         0.769478            0.0001      0.010146   \n",
       "3            0.0         0.827791            0.0001      0.008159   \n",
       "4            0.0         0.834400            0.0001      0.007950   \n",
       "5            0.0         0.853630            0.0001      0.008054   \n",
       "...          ...              ...               ...           ...   \n",
       "8756         0.0         0.041520            0.0001      0.315900   \n",
       "8757         0.0         0.046676            0.0001      0.280545   \n",
       "8758         0.0         0.042982            0.0001      0.154498   \n",
       "8759         0.0         0.037774            0.0001      0.048222   \n",
       "8760         0.0         0.037774            0.0001      0.023222   \n",
       "\n",
       "           HYDRO_RIVER_SO  SOLAR_SO  D_of_H  H_of_D  \n",
       "{PERIODS}                                            \n",
       "1                0.219106       0.0       1       1  \n",
       "2                0.196958       0.0       1       2  \n",
       "3                0.183511       0.0       1       3  \n",
       "4                0.175827       0.0       1       4  \n",
       "5                0.173228       0.0       1       5  \n",
       "...                   ...       ...     ...     ...  \n",
       "8756             0.329281       0.0     365      20  \n",
       "8757             0.282499       0.0     365      21  \n",
       "8758             0.234813       0.0     365      22  \n",
       "8759             0.180121       0.0     365      23  \n",
       "8760             0.155827       0.0     365      24  \n",
       "\n",
       "[8760 rows x 35 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building timeseries from TDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c='NO'\n",
    "name = 'SOLAR'+'_'+c\n",
    "ts = all_TD_ts[name]\n",
    "ts.columns = np.arange(1,Nbr_TD+1) \n",
    "ts = ts*norm[name]/norm_TD[name]\n",
    "ts.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_NO = pd.DataFrame(np.zeros((8760,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in solar_NO.index:\n",
    "    solar_NO.loc[t,'SOLAR'] =  ts.loc[T_H_TD.loc[t,'H_of_D'],T_H_TD.loc[t,'TD_of_day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TD_solar_NO_Borasio = pd.read_csv('TD_dec_solar_NO_Borasio.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
